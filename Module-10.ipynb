{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Module 10 Large Language Models**\n",
    "\n",
    "**Exercise-1**\n",
    "\n",
    "**Title:**Generating Text Using GPT-2 Language Model\n",
    "\n",
    "**Problem Statement:**\n",
    "The goal is to demonstrate how to use the GPT-2 language model to generate text based on an input prompt.\n",
    "\n",
    "**Steps to Follow:**\n",
    "\n",
    "**1.\tImport Libraries:** Import necessary libraries including GPT2LMHeadModel and GPT2Tokenizer from the Transformers library.\n",
    "\n",
    "**2.\tLoad Pre-trained Model and Tokenizer:** Initialize the GPT-2 tokenizer and model using the specified pre-trained model (gpt2 in this case).\n",
    "\n",
    "**3.\tSet Seed (Optional):** Set a seed for reproducibility if needed. This ensures that the results are consistent across different runs.\n",
    "\n",
    "**4.\tDefine Input Prompt:** Specify the input text prompt that will be used to generate text.\n",
    "\n",
    "**5.\tTokenize Input Text:** Use the tokenizer to convert the input text into token IDs (input_ids), which the model can understand.\n",
    "\n",
    "**6.\tGenerate Text:** Utilize the pre-trained GPT-2 model to generate text based on the input_ids. Parameters like max_length, num_return_sequences, and temperature control the length and randomness of the generated text.\n",
    "\n",
    "**7.\tDecode Generated Output:** Decode the generated token IDs back into human-readable text, skipping any special tokens like padding or end-of-sequence markers.\n",
    "\n",
    "**8.\tPrint Generated Text:** Display the generated text to the user.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a536fc5e8413470082d4f1a1b08a1a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848a119124bb4afcbb20ca2045adeec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d057a7c0a54707a32d7be43dec5216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d930827398ef493693dad88bc4760c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "688f36bd86264cbe8c32953dd12ccde1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab035aba7fa457ab8c20a326a7d19a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc1ba58783e64401aeceedc05ea958f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"  # Specify the pre-trained model name\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)  # Initialize tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)  # Initialize model\n",
    "\n",
    "# Set seed for reproducibility (optional)\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Input prompt\n",
    "input_text = \"Once upon a time\"\n",
    "\n",
    "# Tokenize input text\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')  # Convert input text to token IDs\n",
    "\n",
    "# Generate text based on input prompt\n",
    "output = model.generate(input_ids, max_length=50, num_return_sequences=1, temperature=0.7)\n",
    "# Generate text using the model. Parameters:\n",
    "#   - input_ids: Token IDs of the input text\n",
    "#   - max_length: Maximum length of the generated text\n",
    "#   - num_return_sequences: Number of sequences to generate\n",
    "#   - temperature: Controls the randomness of the generation. Higher values result in more random output.\n",
    "\n",
    "# Decode generated output\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "# Decode the generated token IDs to text, removing any special tokens like padding or EOS.\n",
    "\n",
    "# Print generated text\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "**1.\tImport Libraries:** Import necessary modules from the Transformers library to work with GPT-2.\n",
    "\n",
    "**2.\tLoad Pre-trained Model and Tokenizer:** Initialize the tokenizer and model with the gpt2 pre-trained model.\n",
    "\n",
    "**3.\tSet Seed:** Set a seed using torch.manual_seed(42) for reproducibility of results.\n",
    "\n",
    "**4.\tInput Prompt:** Define an initial text prompt (\"Once upon a time\" in this case).\n",
    "\n",
    "**5.\tTokenization:** Use the tokenizer to convert the input text into token IDs (input_ids).\n",
    "\n",
    "**6.\tText Generation:** Use the GPT-2 model to generate text based on input_ids, controlling generation parameters like max_length and temperature.\n",
    "\n",
    "**7.\tDecoding:** Convert the generated token IDs back into readable text, skipping special tokens.\n",
    "\n",
    "**8.\tOutput:** Print the generated text to the console.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise-2**\n",
    "\n",
    "**Title:** Generating Text Styles and Translating Languages Using Pre-trained Models\n",
    "\n",
    "**Problem Statement:**\n",
    "Demonstrate how to use pre-trained language models for generating text in different styles (creative output) and performing language translation without relying on external APIs.\n",
    "\n",
    "**Steps to Follow:**\n",
    "\n",
    "1.\tImport Necessary Libraries:\n",
    "\n",
    "    a.\tImport transformers for using pre-trained models.\n",
    "\n",
    "    b.\tImport AutoTokenizer and AutoModelForSeq2SeqLM from transformers to access model architectures and tokenizers.\n",
    "\n",
    "2.\tDefine Functions for Text Generation and Translation:\n",
    "\n",
    "    a.\tgenerate_text_styling(prompt_text, model_name, max_length=50, temperature=0.9):\n",
    "\n",
    "            i.\tLoad the model and tokenizer using AutoModelForCausalLM and AutoTokenizer.\n",
    "            ii.\tTokenize the input text.\n",
    "            iii.\tGenerate text based on the input prompt using the model with specified parameters.\n",
    "            iv.\tDecode the generated token IDs to text.\n",
    "\n",
    "    b.\ttranslate_text(input_text, target_language, model_name, max_length=100):\n",
    "            i.\tLoad the translation model and tokenizer using AutoModelForSeq2SeqLM and AutoTokenizer.\n",
    "            ii.\tTokenize the input text in the source language.\n",
    "            iii.\tGenerate translated text into the target language using the model with specified parameters.\n",
    "            iv.\tDecode the generated token IDs to text.\n",
    "3.\tExample Usage:\n",
    "\n",
    "    a.\tgenerate_text_styling:\n",
    "            i.\tProvide a prompt text.\n",
    "            ii.\tSpecify the model name, max_length, and temperature for generating creative output.\n",
    "    b.\ttranslate_text:\n",
    "            i.\tProvide input text in the source language.\n",
    "            ii.\tSpecify the target language, model name, and max_length for translation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creative Output:\n",
      "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a\n",
      "\n",
      "Translation Output (French):\n",
      "Le chat sat on the mat.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "\n",
    "def generate_text_styling(prompt_text, model_name=\"gpt2\", max_length=50, temperature=0.9):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    input_ids = tokenizer.encode(prompt_text, return_tensors='pt')\n",
    "    output = model.generate(input_ids, max_length=max_length, temperature=temperature, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "def translate_text(input_text, target_language, model_name=\"t5-small\", max_length=100):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "    translated_output = model.generate(input_ids, max_length=max_length, num_beams=4, early_stopping=True)\n",
    "\n",
    "    translated_text = tokenizer.decode(translated_output[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# Example of generating creative output\n",
    "prompt_text = \"Once upon a time\"\n",
    "creative_output = generate_text_styling(prompt_text)\n",
    "print(\"Creative Output:\")\n",
    "print(creative_output)\n",
    "\n",
    "# Example of translating text\n",
    "input_text_english = \"The cat sat on the mat.\"\n",
    "translated_text = translate_text(input_text_english, target_language=\"fr\")\n",
    "print(\"\\nTranslation Output (French):\")\n",
    "print(translated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "1.\tImporting Libraries: Import necessary libraries from transformers for loading and using pre-trained models (AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM).\n",
    "\n",
    "2.\tFunctions:\n",
    "\n",
    "      a.\tgenerate_text_styling: Loads a GPT-2 model specified by model_name, generates text based on prompt_text with max_length and temperature.\n",
    "\n",
    "      b.\ttranslate_text: Loads a T5 model specified by model_name, translates input_text from English to target_language (specified), with max_length for translation.\n",
    "\n",
    "3.\tExample Usage: Demonstrates how to use these functions for generating creative output and translating text without relying on an external API key.\n",
    "\n",
    "\n",
    "This approach uses Hugging Face's transformers library to access and utilize pre-trained models directly within your Python environment, ensuring flexibility and ease of use for various NLP tasks.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
