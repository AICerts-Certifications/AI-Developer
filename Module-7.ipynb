{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Module 7: Natural Language Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise-1**\n",
    "\n",
    "**Title:** Cleaning and Preparing Text Data for NLP Tasks\n",
    "\n",
    "**Problem Statement:**\n",
    "The goal of this task is to clean and preprocess a given text dataset to make it suitable for natural language processing (NLP) tasks. This includes removing punctuation, numbers, converting text to lowercase, tokenizing the text, and removing stopwords. Proper preprocessing of text data is crucial to improve the performance of NLP models.\n",
    "\n",
    "**Steps to be Followed:**\n",
    "\n",
    "1.\tInstall and Import Necessary Libraries:\n",
    "\n",
    "    a.\tInstall the NLTK library.\n",
    "\n",
    "    b.\tImport necessary modules from NLTK and other libraries.\n",
    "\n",
    "2.\tDownload NLTK Resources:\n",
    "\n",
    "    a.\tDownload the stopwords and punkt resources from NLTK.\n",
    "\n",
    "3.\tDefine and Clean the Text Data:\n",
    "\n",
    "    a.\tCreate a sample text data string.\n",
    "\n",
    "    b.\tDefine a function to clean the text by removing punctuation, numbers, and converting to lowercase.\n",
    "\n",
    "4.\tTokenize the Cleaned Text:\n",
    "\n",
    "    a.\tDefine a function to tokenize the cleaned text using NLTK's word_tokenize method.\n",
    "\n",
    "5.\tRemove Stopwords:\n",
    "\n",
    "    a.\tDefine a function to remove stopwords from the tokenized text.\n",
    "\n",
    "6.\tDisplay the Results:\n",
    "\n",
    "    a.\tPrint the original text, cleaned text, tokenized text, and text after stopword removal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: This is a sample text! It includes punctuation, numbers like 123, and stopwords such as the, is, and a.\n",
      "Cleaned Text: this is a sample text it includes punctuation numbers like  and stopwords such as the is and a\n",
      "Tokenized Text: ['this', 'is', 'a', 'sample', 'text', 'it', 'includes', 'punctuation', 'numbers', 'like', 'and', 'stopwords', 'such', 'as', 'the', 'is', 'and', 'a']\n",
      "Text after Stopword Removal: ['sample', 'text', 'includes', 'punctuation', 'numbers', 'like', 'stopwords']\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install and Import Necessary Libraries\n",
    "!pip install nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Step 2: Download Required NLTK Resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample text data\n",
    "text_data = \"This is a sample text! It includes punctuation, numbers like 123, and stopwords such as the, is, and a.\"\n",
    "\n",
    "# Step 3: Define a Function to Clean Text\n",
    "def clean_text(text):\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Step 4: Define a Function to Tokenize Text\n",
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Step 5: Define a Function to Remove Stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "# Clean the text\n",
    "cleaned_text = clean_text(text_data)\n",
    "# Tokenize the cleaned text\n",
    "tokens = tokenize_text(cleaned_text)\n",
    "# Remove stopwords from the tokenized text\n",
    "filtered_tokens = remove_stopwords(tokens)\n",
    "\n",
    "# Step 6: Print the Results\n",
    "print(\"Original Text:\", text_data)\n",
    "print(\"Cleaned Text:\", cleaned_text)\n",
    "print(\"Tokenized Text:\", tokens)\n",
    "print(\"Text after Stopword Removal:\", filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of the Code:**\n",
    "\n",
    "1.\tInstall and Import Necessary Libraries:\n",
    "\n",
    "    a.\tThe nltk library is installed and necessary modules like stopwords and re (for regular expressions) are imported.\n",
    "\n",
    "2.\tDownload Required NLTK Resources:\n",
    "\n",
    "    a.\tStopwords and punkt resources are downloaded using nltk.download to ensure that stopword removal and tokenization can be performed.\n",
    "\n",
    "3.\tClean Text:\n",
    "\n",
    "    a.\tThe clean_text function removes punctuation using a regular expression that matches any character that is not a word character or whitespace.\n",
    "\n",
    "    b.\tNumbers are removed using another regular expression that matches digits.\n",
    "\n",
    "    c.\tThe text is converted to lowercase to ensure uniformity.\n",
    "\n",
    "4.\tTokenize Text:\n",
    "\n",
    "    a.\tThe tokenize_text function uses nltk.word_tokenize to split the text into individual words (tokens).\n",
    "\n",
    "5.\tRemove Stopwords:\n",
    "\n",
    "    a.\tThe remove_stopwords function filters out common English stopwords from the tokenized text using a list comprehension and the set of stopwords provided by NLTK.\n",
    "\n",
    "6.\tPrint the Results:\n",
    "\n",
    "    a.\tThe original text, cleaned text, tokenized text, and the text after stopword removal are printed to verify the results of each preprocessing step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise-2**\n",
    "\n",
    "**Title:** Sentiment Analysis Using Naive Bayes Classifier\n",
    "\n",
    "**Problem Statement:**\n",
    "The goal is to build a sentiment analysis model that classifies movie reviews as either positive or negative. Using the NLTK library, we will train a Naive Bayes classifier on the movie reviews dataset and evaluate its performance. Additionally, the model will be tested on a custom review to predict its sentiment.\n",
    "\n",
    "**Steps to be Followed:**\n",
    "\n",
    "1.\tInstall and Import Necessary Libraries:\n",
    "\n",
    "    a.\tImport modules from nltk.\n",
    "\n",
    "    b.\tImport additional necessary functions like shuffle.\n",
    "\n",
    "2.\tDownload Required NLTK Resources:\n",
    "\n",
    "    a.\tDownload the movie reviews dataset and stopwords.\n",
    "\n",
    "3.\tLoad and Prepare Data:\n",
    "\n",
    "    a.\tLoad positive and negative movie reviews.\n",
    "\n",
    "    b.\tCombine and shuffle the dataset.\n",
    "\n",
    "4.\tExtract Features:\n",
    "\n",
    "    a.\tExtract the most frequent words from the dataset to use as features.\n",
    "\n",
    "    b.\tDefine a function to extract features from each review.\n",
    "\n",
    "5.\tCreate Training and Testing Datasets:\n",
    "\n",
    "    a.\tSplit the dataset into training and testing sets.\n",
    "\n",
    "6.\tTrain the Naive Bayes Classifier:\n",
    "\n",
    "    a.\tTrain the classifier using the training set.\n",
    "\n",
    "7.\tEvaluate the Classifier:\n",
    "\n",
    "    a.\tEvaluate the classifier on the test set and print the accuracy.\n",
    "\n",
    "8.\tTest on Custom Review:\n",
    "\n",
    "    a.\tTest the classifier on a custom review and print the predicted sentiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.40%\n",
      "Sentiment of the custom review: negative\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install and Import Necessary Libraries\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy as nltk_accuracy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from random import shuffle\n",
    "\n",
    "# Step 2: Download Required NLTK Resources\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Step 3: Load and Prepare Data\n",
    "# Load positive and negative movie reviews\n",
    "positive_reviews = [(movie_reviews.words(file_id), 'positive') for file_id in movie_reviews.fileids('pos')]\n",
    "negative_reviews = [(movie_reviews.words(file_id), 'negative') for file_id in movie_reviews.fileids('neg')]\n",
    "\n",
    "# Combine positive and negative reviews and shuffle the dataset\n",
    "all_reviews = positive_reviews + negative_reviews\n",
    "shuffle(all_reviews)\n",
    "\n",
    "# Step 4: Extract Features\n",
    "# Extract the most frequent words as features\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "word_features = list(all_words)[:2000]\n",
    "\n",
    "# Define a function to extract features from the review\n",
    "def extract_features(review):\n",
    "    words = set(review)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[word] = (word in words)\n",
    "    return features\n",
    "\n",
    "# Step 5: Create Training and Testing Datasets\n",
    "featuresets = [(extract_features(review), sentiment) for (review, sentiment) in all_reviews]\n",
    "train_set, test_set = featuresets[:1500], featuresets[1500:]\n",
    "\n",
    "# Step 6: Train the Naive Bayes Classifier\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# Step 7: Evaluate the Classifier\n",
    "accuracy = nltk_accuracy(classifier, test_set)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Step 8: Test on Custom Review\n",
    "custom_review = \"This movie was fantastic! I loved every moment of it.\"\n",
    "custom_tokens = word_tokenize(custom_review.lower())\n",
    "custom_features = extract_features(custom_tokens)\n",
    "sentiment = classifier.classify(custom_features)\n",
    "print(f'Sentiment of the custom review: {sentiment}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of the Code:**\n",
    "1.\tInstall and Import Necessary Libraries:\n",
    "\n",
    "    a.\tThe necessary modules from the nltk library are imported along with shuffle from the random module.\n",
    "\n",
    "2.\tDownload Required NLTK Resources:\n",
    "\n",
    "    a.\tThe movie reviews dataset and stopwords are downloaded to ensure they are available for use.\n",
    "\n",
    "3.\tLoad and Prepare Data:\n",
    "\n",
    "    a.\tPositive and negative movie reviews are loaded from the movie reviews dataset.\n",
    "\n",
    "    b.\tThe reviews are combined into a single dataset and shuffled to ensure randomness.\n",
    "\n",
    "4.\tExtract Features:\n",
    "\n",
    "    a.\tThe most frequent words from the dataset are extracted to use as features.\n",
    "\n",
    "    b.\tA function extract_features is defined to extract these features from each review, creating a feature dictionary for the classifier.\n",
    "\n",
    "5.\tCreate Training and Testing Datasets:\n",
    "\n",
    "    a.\tThe dataset is split into training and testing sets, with the training set consisting of the first 1500 reviews and the testing set consisting of the remaining reviews.\n",
    "\n",
    "6.\tTrain the Naive Bayes Classifier:\n",
    "\n",
    "    a.\tA Naive Bayes classifier is trained using the training set.\n",
    "\n",
    "7.\tEvaluate the Classifier:\n",
    "\n",
    "    a.\tThe classifier is evaluated on the test set, and the accuracy is printed.\n",
    "\n",
    "8.\tTest on Custom Review:\n",
    "\n",
    "    a.\tThe classifier is tested on a custom review to predict its sentiment, and the predicted sentiment is printed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise-3**\n",
    "\n",
    "**Title:** Building a Sentiment Analyzer for Social Media Posts\n",
    "\n",
    "**Problem Statement:**\n",
    "The task is to build a sentiment analysis model that classifies social media posts as either positive or negative. Using a dataset of social media posts, we will preprocess the text data, transform it using TF-IDF vectorization, and train a logistic regression model. The model's performance will be evaluated using accuracy and classification metrics.\n",
    "\n",
    "**Steps to be Followed:**\n",
    "\n",
    "1.\tInstall and Import Necessary Libraries:\n",
    "\n",
    "    a.\tImport necessary libraries for data handling, model training, and evaluation.\n",
    "\n",
    "2.\tLoad and Display Dataset:\n",
    "\n",
    "    a.\tCreate the sample data.\n",
    "\n",
    "    b.\tDisplay the first few rows of the dataset to understand its structure.\n",
    "\n",
    "3.\tDefine Text Cleaning Function:\n",
    "\n",
    "    a.\tDefine a function to clean the text data by removing URLs, mentions, hashtags, digits, punctuation, and stopwords, and converting the text to lowercase.\n",
    "\n",
    "4.\tApply Text Cleaning Function:\n",
    "\n",
    "    a.\tApply the cleaning function to the text data in the dataset.\n",
    "\n",
    "5.\tSplit Data into Features and Target Labels:\n",
    "\n",
    "    a.\tSplit the data into features (X) and target labels (y).\n",
    "\n",
    "6.\tSplit Data into Training and Testing Sets:\n",
    "\n",
    "    a.\tSplit the data into training and testing sets using a defined test size and random state.\n",
    "\n",
    "7.\tInitialize and Fit TF-IDF Vectorizer:\n",
    "\n",
    "    a.\tInitialize the TF-IDF vectorizer with a specified maximum number of features.\n",
    "\n",
    "    b.\tFit and transform the training data using the TF-IDF vectorizer.\n",
    "\n",
    "    c.\tTransform the testing data using the fitted TF-IDF vectorizer.\n",
    "\n",
    "8.\tInitialize and Train Logistic Regression Model:\n",
    "\n",
    "    a.\tInitialize the logistic regression model with an increased maximum number of iterations for convergence.\n",
    "\n",
    "    b.\tTrain the model on the TF-IDF transformed training data.\n",
    "\n",
    "9.\tPredict and Evaluate the Model:\n",
    "\n",
    "    a.\tPredict sentiment labels for the testing data.\n",
    "\n",
    "    b.\tCalculate and print the accuracy of the model.\n",
    "\n",
    "    c.\tDisplay the classification report to show detailed performance metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00         1\n",
      "    positive       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         2\n",
      "   macro avg       1.00      1.00      1.00         2\n",
      "weighted avg       1.00      1.00      1.00         2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install and Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK stopwords if not already present\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Step 2: Create and Load Sample Dataset\n",
    "data = {\n",
    "    'Text': [\n",
    "        \"I love the new design of the app! It's fantastic and user-friendly.\",\n",
    "        \"The recent update is terrible. It crashes all the time.\",\n",
    "        \"Great job on the new features, very innovative!\",\n",
    "        \"I'm really disappointed with the customer service.\",\n",
    "        \"This is the best product I've ever used!\",\n",
    "        \"Not happy with the performance of the new version.\",\n",
    "        \"Absolutely amazing experience, I highly recommend it!\",\n",
    "        \"The interface is confusing and not intuitive at all.\",\n",
    "        \"Fantastic support from the team, helped me resolve my issue quickly.\",\n",
    "        \"Horrible experience, I want a refund.\"\n",
    "    ],\n",
    "    'Sentiment': [\n",
    "        'positive', 'negative', 'positive', 'negative', 'positive',\n",
    "        'negative', 'positive', 'negative', 'positive', 'negative'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 3: Define Text Cleaning Function\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)     # Remove mentions\n",
    "    text = re.sub(r'#\\w+', '', text)     # Remove hashtags\n",
    "    text = re.sub(r'\\d+', '', text)      # Remove digits\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = text.lower()                  # Convert to lowercase\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords.words('english')])  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "# Step 4: Apply Text Cleaning Function\n",
    "df['Text'] = df['Text'].apply(clean_text)\n",
    "\n",
    "# Step 5: Split Data into Features and Target Labels\n",
    "X = df['Text']\n",
    "y = df['Sentiment']\n",
    "\n",
    "# Step 6: Split Data into Training and Testing Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 7: Initialize and Fit TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "\n",
    "# Fit and transform training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform testing data\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Step 8: Initialize and Train Logistic Regression Model\n",
    "model = LogisticRegression(max_iter=200)  # Increased max_iter for convergence\n",
    "\n",
    "# Train model on training data\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Step 9: Predict and Evaluate the Model\n",
    "# Predict sentiment labels for testing data\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Display classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of the Code:**\n",
    "1.\tInstall and Import Necessary Libraries:\n",
    "\n",
    "    a.\tpandas for data handling.\n",
    "\n",
    "    b.\tre for regular expressions.\n",
    "\n",
    "    c.\ttrain_test_split, TfidfVectorizer, LogisticRegression, accuracy_score, and classification_report from sklearn for model training and evaluation.\n",
    "\n",
    "    d.\tnltk for natural language processing tasks.\n",
    "\n",
    "2.\tCreate and Load Sample Dataset:\n",
    "\n",
    "    a.\tThe dataset is created with sample text and sentiment labels, mimicking social media posts.\n",
    "\n",
    "3.\tDefine Text Cleaning Function:\n",
    "\n",
    "    a.\tThe clean_text function removes unwanted elements from the text, such as URLs, mentions, hashtags, digits, punctuation, and stopwords, and converts the text to lowercase.\n",
    "\n",
    "4.\tApply Text Cleaning Function:\n",
    "\n",
    "    a.\tThe cleaning function is applied to the Text column of the dataset to preprocess the text data.\n",
    "\n",
    "5.\tSplit Data into Features and Target Labels:\n",
    "\n",
    "    a.\tThe Text column is used as features (X), and the Sentiment column is used as target labels (y).\n",
    "\n",
    "6.\tSplit Data into Training and Testing Sets:\n",
    "\n",
    "    a.\tThe data is split into training and testing sets with a test size of 20% and a random state of 42 for reproducibility.\n",
    "\n",
    "7.\tInitialize and Fit TF-IDF Vectorizer:\n",
    "\n",
    "    a.\tThe TF-IDF vectorizer is initialized with a maximum of 1000 features.\n",
    "\n",
    "    b.\tThe training data is fit and transformed, and the testing data is transformed using the fitted vectorizer.\n",
    "\n",
    "8.\tInitialize and Train Logistic Regression Model:\n",
    "\n",
    "    a.\tThe logistic regression model is initialized with an increased maximum number of iterations (200) for convergence.\n",
    "\n",
    "    b.\tThe model is trained on the TF-IDF transformed training data.\n",
    "\n",
    "9.\tPredict and Evaluate the Model:\n",
    "\n",
    "    a.\tThe model predicts sentiment labels for the testing data.\n",
    "\n",
    "    b.\tThe accuracy of the model is calculated and printed.\n",
    "\n",
    "    c.\tA classification report is displayed, showing detailed performance metrics such as precision, recall, and F1-score for each class (positive and negative).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise-4**\n",
    "\n",
    "**Title**: BERT Implementation for Question-Answering\n",
    "\n",
    "**Problem Statement:**\n",
    "Implement a BERT-based model to perform question-answering tasks, allowing users to ask questions based on a given context and receive accurate answers. This implementation demonstrates how to use the Hugging Face transformers library to load a pre-trained BERT model and tokenizer for this purpose.\n",
    "\n",
    "**Steps to be Followed:**\n",
    "1.\tInstall Required Libraries:\n",
    "\n",
    "    a.\tInstall the transformers library.\n",
    "2.\tImport Necessary Modules:\n",
    "\n",
    "    a.\tImport the pipeline function from the transformers library.\n",
    "3.\tLoad the Question-Answering Pipeline:\n",
    "\n",
    "    a.\tInitialize the question-answering pipeline with a pre-trained BERT model and tokenizer.\n",
    "4.\tDefine the Context and Questions:\n",
    "\n",
    "    a.\tProvide the context (text) from which the model will extract answers.\n",
    "    \n",
    "    b.\tDefine the questions to be asked based on the context.\n",
    "5.\tPerform Question-Answering:\n",
    "\n",
    "    a.\tUse the pipeline to answer the questions based on the given context.\n",
    "6.\tDisplay the Results:\n",
    "\n",
    "    a.\tPrint the questions and their corresponding answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are transformers ideal for?\n",
      "Answer: that process sequential data, making them ideal for tasks such\n",
      "\n",
      "Question: What is BERT?\n",
      "Answer: that process sequential data, making them ideal for tasks such\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install Required Libraries\n",
    "# Uncomment the line below if running in an environment where transformers is not installed\n",
    "# !pip install transformers\n",
    "\n",
    "# Step 2: Import Necessary Modules\n",
    "from transformers import pipeline\n",
    "\n",
    "# Step 3: Load the Question-Answering Pipeline\n",
    "qa_pipeline = pipeline('question-answering', model='bert-base-uncased', tokenizer='bert-base-uncased')\n",
    "\n",
    "# Step 4: Define the Context and Questions\n",
    "context = \"\"\"\n",
    "Transformers are models that process sequential data, making them ideal for tasks such as language translation and text summarization.\n",
    "BERT, which stands for Bidirectional Encoder Representations from Transformers, is a transformer-based model designed to understand the context of words in a sentence.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"What are transformers ideal for?\",\n",
    "    \"What is BERT?\"\n",
    "]\n",
    "\n",
    "# Step 5: Perform Question-Answering\n",
    "for question in questions:\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {result['answer']}\\n\")\n",
    "\n",
    "# Step 6: Display the Results\n",
    "# The results are printed within the loop above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of the Code:**\n",
    "1.\tInstall Required Libraries:\n",
    "\n",
    "    a.\tThe transformers library is required to use pre-trained BERT models. The installation command is commented out but can be uncommented if needed.\n",
    "\n",
    "2.\tImport Necessary Modules:\n",
    "\n",
    "    a.\tThe pipeline function from the transformers library is imported. This function simplifies the process of using various models for different tasks, such as question-answering.\n",
    "\n",
    "3.\tLoad the Question-Answering Pipeline:\n",
    "\n",
    "    a.\tThe pipeline function is used to create a question-answering pipeline with the bert-base-uncased model and tokenizer. This pre-trained BERT model is suitable for English language tasks.\n",
    "\n",
    "4.\tDefine the Context and Questions:\n",
    "\n",
    "    a.\tThe context is a string containing the text from which the model will extract answers.\n",
    "\n",
    "    b.\tA list of questions related to the context is defined.\n",
    "\n",
    "5.\tPerform Question-Answering:\n",
    "\n",
    "    a.\tA loop iterates over each question, and the qa_pipeline is used to generate answers based on the provided context.\n",
    "\n",
    "    b.\tThe results, including the questions and their corresponding answers, are printed.\n",
    "\n",
    "6.\tDisplay the Results:\n",
    "\n",
    "    a.\tThe results are displayed within the loop, providing a clear and concise output for each question-answer pair.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise-5**\n",
    "\n",
    "**Title:** T5 Implementation for Text Summarization\n",
    "\n",
    "**Problem Statement:**\n",
    "Implement a text summarization solution using the T5 (Text-To-Text Transfer Transformer) model. T5 is designed to handle various natural language processing (NLP) tasks by treating both input and output as text, allowing for a unified approach to different tasks. This implementation will demonstrate how to use the Hugging Face transformers library to load a pre-trained T5 model and tokenizer for summarizing a given text.\n",
    "\n",
    "**Steps to be Followed:**\n",
    "1.\tInstall Required Libraries:\n",
    "\n",
    "    a.\tEnsure the transformers library is installed.\n",
    "\n",
    "2.\tImport Necessary Modules:\n",
    "\n",
    "    a.\tImport T5ForConditionalGeneration and T5Tokenizer from the transformers library.\n",
    "\n",
    "3.\tLoad the T5 Model and Tokenizer:\n",
    "\n",
    "    a.\tInitialize the T5 model and tokenizer using the pre-trained t5-base model.\n",
    "\n",
    "4.\tDefine the Text for Summarization:\n",
    "\n",
    "    a.\tProvide the text that needs to be summarized.\n",
    "\n",
    "5.\tPreprocess the Input Text:\n",
    "\n",
    "    a.\tEncode the input text using the T5 tokenizer, adding a task prefix and setting the appropriate length constraints.\n",
    "\n",
    "6.\tGenerate the Summary:\n",
    "\n",
    "    a.\tUse the T5 model to generate a summary from the encoded input text.\n",
    "\n",
    "7.\tDecode and Display the Summary:\n",
    "\n",
    "    a.\tDecode the generated summary and print both the original text and the generated summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e686f31d88427a815311f537f9eedb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4866a315f3c48598745f468c29eeace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "589506b751ee4cdd973fd52c03d74504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0f05a61e9df49608f3e43b4bfd8fb6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "898095ef5d05437a858ef2b04018efb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      " \n",
      "T5 (Text-To-Text Transfer Transformer) is a transformer-based model introduced by Google Research. \n",
      "It is designed for various natural language processing tasks by formulating them as text-to-text problems. \n",
      "In T5, input and output are treated as text, allowing the model to handle various tasks in a unified manner.\n",
      "\n",
      "\n",
      "Generated Summary:\n",
      " Google Research has introduced a transformer-based model for text-to-text problems.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install Required Libraries\n",
    "# Uncomment the line below if running in an environment where transformers is not installed\n",
    "# !pip install transformers\n",
    "\n",
    "# Step 2: Import Necessary Modules\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Step 3: Load the T5 Model and Tokenizer\n",
    "model_name = \"t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Step 4: Define the Text for Summarization\n",
    "text_to_summarize = \"\"\"\n",
    "T5 (Text-To-Text Transfer Transformer) is a transformer-based model introduced by Google Research.\n",
    "It is designed for various natural language processing tasks by formulating them as text-to-text problems.\n",
    "In T5, input and output are treated as text, allowing the model to handle various tasks in a unified manner.\n",
    "\"\"\"\n",
    "\n",
    "# Step 5: Preprocess the Input Text\n",
    "input_ids = tokenizer.encode(\"summarize: \" + text_to_summarize, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "# Step 6: Generate the Summary\n",
    "summary_ids = model.generate(input_ids)\n",
    "\n",
    "# Step 7: Decode and Display the Summary\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(\"Original Text:\\n\", text_to_summarize)\n",
    "print(\"\\nGenerated Summary:\\n\", summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of the Code:**\n",
    "\n",
    "1.\tInstall Required Libraries:\n",
    "\n",
    "    a.\tThe transformers library is necessary to use the T5 model. The installation command is provided but commented out.\n",
    "\n",
    "2.\tImport Necessary Modules:\n",
    "\n",
    "    a.\tThe T5ForConditionalGeneration and T5Tokenizer classes from the transformers library are imported to load and use the T5 model and tokenizer.\n",
    "\n",
    "3.\tLoad the T5 Model and Tokenizer:\n",
    "\n",
    "    a.\tThe pre-trained t5-base model and tokenizer are loaded using their respective classes. The t5-base model is suitable for a variety of text-to-text NLP tasks, including summarization.\n",
    "\n",
    "4.\tDefine the Text for Summarization:\n",
    "\n",
    "    a.\tA sample text is provided that needs to be summarized. This text can be replaced with any other text as needed.\n",
    "\n",
    "5.\tPreprocess the Input Text:\n",
    "\n",
    "    a.\tThe input text is preprocessed by encoding it with the T5 tokenizer. The prefix \"summarize: \" indicates the task. The max_length and truncation parameters ensure the input is appropriately sized for the model.\n",
    "\n",
    "6.\tGenerate the Summary:\n",
    "\n",
    "    a.\tThe T5 model generates a summary from the preprocessed input text. The generate method produces the summary in the form of token IDs.\n",
    "\n",
    "7.\tDecode and Display the Summary:\n",
    "\n",
    "    a.\tThe generated summary is decoded back into text using the tokenizer. Both the original text and the generated summary are printed for comparison.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
